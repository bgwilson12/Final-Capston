{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "(734, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Total Word Count: , This ...</td>\n",
       "      <td>Inquiry into Patient Death</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Leadership has been descr...</td>\n",
       "      <td>Motivation Skills Development Plan for Nursing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Depression among pregnant...</td>\n",
       "      <td>Depression among Pregnant Adolescents: Literat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Davidson, E., Daly, J., B...</td>\n",
       "      <td>Family Support Programme for ICU Patient Relat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Idiopathic pulmonary fibr...</td>\n",
       "      <td>Idiopathic Pulmonary Fibrosis: An Overview</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject                                               text  \\\n",
       "0  Nursing   [\\n, \\n, \\n, \\n, \\n, Total Word Count: , This ...   \n",
       "1  Nursing   [\\n, \\n, \\n, \\n, \\n, Leadership has been descr...   \n",
       "2  Nursing   [\\n, \\n, \\n, \\n, \\n, Depression among pregnant...   \n",
       "3  Nursing   [\\n, \\n, \\n, \\n, \\n, Davidson, E., Daly, J., B...   \n",
       "4  Nursing   [\\n, \\n, \\n, \\n, \\n, Idiopathic pulmonary fibr...   \n",
       "\n",
       "                                               title  student  \n",
       "0                         Inquiry into Patient Death        1  \n",
       "1     Motivation Skills Development Plan for Nursing        1  \n",
       "2  Depression among Pregnant Adolescents: Literat...        1  \n",
       "3  Family Support Programme for ICU Patient Relat...        1  \n",
       "4         Idiopathic Pulmonary Fibrosis: An Overview        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This let's me iterate faster by deleting it at the beginning\n",
    "import os\n",
    "# os.remove('../uk_students.json')\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class EssayItem(scrapy.Item):\n",
    "    subject = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    text = scrapy.Field()\n",
    "\n",
    "class SESpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"SES\" # for student essay spider\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.ukessays.com/essays/',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response from the start_urls we declared.\n",
    "    def parse(self, response):\n",
    "        # Only pull essays from the \"Big Three\"\n",
    "        subject_list = ['Business ', 'Law ', 'Nursing ']\n",
    "        \n",
    "        # Iterate over every column element on the page.\n",
    "        for subjects in response.xpath('//div[@class=\"col-lg-3 col-sm-6\"]/div/ul/li'):\n",
    "            \n",
    "            if subjects.xpath('a/text()').extract_first() not in subject_list:\n",
    "                continue\n",
    "                \n",
    "            # Create our subject from this top-level page\n",
    "            subject = subjects.xpath('a/text()').extract_first()\n",
    "            \n",
    "            # Create an EssayItem called essay_item, empty for now\n",
    "            essay_item = EssayItem()\n",
    "            \n",
    "            # Create new url for parse_essays to use\n",
    "            essay_list_url = response.urljoin(subjects.xpath('a/@href').extract_first())\n",
    "            \n",
    "            # New request with essay_list_url, pass in our essay_item\n",
    "            yield scrapy.Request(essay_list_url, callback=self.parse_essays, \n",
    "                                 dont_filter=True,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject})\n",
    "            \n",
    "    def parse_essays(self, response):\n",
    "\n",
    "        # Retrieve essay items from metadata\n",
    "        essay_item = response.request.meta['item']\n",
    "        subject = response.request.meta['subject']\n",
    "        \n",
    "        for i, essay in enumerate(response.xpath('//div/article/div/h4')):\n",
    "                \n",
    "            # Xpath to the essay's title\n",
    "            title = essay.xpath('a/text()').extract_first()\n",
    "            \n",
    "            # Xpath into the actual essay's link... finally!\n",
    "            essay_url = response.urljoin(essay.xpath('a/@href').extract_first())\n",
    "            \n",
    "            # This one should return the actual text, along with the rest of the item fields\n",
    "            yield scrapy.Request(essay_url, callback=self.collect_essay,\n",
    "                                 dont_filter=False,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject,\n",
    "                                       'title':title})\n",
    "            \n",
    "        # After loop, update the page number\n",
    "        page_num = int(response.xpath('//li[@class=\"page-item active\"]/span/text()').extract_first())\n",
    "        \n",
    "        if page_num < 7:\n",
    "            next_page = '?page={}'.format(page_num+1)\n",
    "            next_url = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_url, callback=self.parse_essays,\n",
    "                                 dont_filter=False,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject})\n",
    "        \n",
    "        \n",
    "    def collect_essay(self, response):\n",
    "        # Retrieve our essay_item, once again, from Response's metadata\n",
    "        essay_item = response.request.meta['item']\n",
    "                \n",
    "        # Add the text, subject, and title into our essay_item\n",
    "        # Something weird was happening, probably due to the aysync nature of this stuff\n",
    "        essay_item['text'] = response.xpath('//body/main/div/div/article/p/text()').extract()\n",
    "        essay_item['subject'] = response.request.meta['subject']\n",
    "        essay_item['title'] = response.request.meta['title']        \n",
    "        \n",
    "        yield essay_item\n",
    "        \n",
    "        \n",
    "        \n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT' : 'json',         # Store data in JSON format.\n",
    "    'FEED_URI' : '../uk_students.json',  # Name our storage file.\n",
    "    'LOG_ENABLED' : False,           # Turn off logging for now.\n",
    "    'AUTOTHROTTLE_ENABLED' : True,\n",
    "    'HTTPCACHE_ENABLED' : True,\n",
    "    'ROBOTSTXT_ENABLED' : False,\n",
    "    'DOWNLOAD_DELAY' : 2\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(SESpider)\n",
    "process.start()\n",
    "print('Success!')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "uk_students = pd.read_json('../uk_students.json', orient='records', encoding='latin')\n",
    "uk_students['student'] = 1\n",
    "print(uk_students.shape)\n",
    "uk_students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uk_students.title.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(uk_students, open('../uk_students.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo!! All done with that one. On the the harder part: finding proffessional samples. The websites I've found generally do not post very many samples, unlike this UK Student Sample Bank. There were tens of thousands of essays to choose from, but the same website only posted 40 or so samples from their professionals. I will have to manually find a bunch of websites and make scrapers for all of them :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
