{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "(675, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ecommerce</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Face of consumerism throu...</td>\n",
       "      <td>Impact of E-commerce on the Hospitality Industry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drama</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n,  ped importance, price el...</td>\n",
       "      <td>Importance of Elasticity Demand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Design</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, To export a reference to ...</td>\n",
       "      <td>Posters of the Second World War - An Analysis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analysis</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, When we talk about the Ma...</td>\n",
       "      <td>Mathematical description of OFDM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cultural Studies</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, The culture of the educat...</td>\n",
       "      <td>Cultural Analysis of Pakistan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subject                                               text  \\\n",
       "0         Ecommerce   [\\n, \\n, \\n, \\n, \\n, Face of consumerism throu...   \n",
       "1             Drama   [\\n, \\n, \\n, \\n, \\n,  ped importance, price el...   \n",
       "2            Design   [\\n, \\n, \\n, \\n, \\n, To export a reference to ...   \n",
       "3     Data Analysis   [\\n, \\n, \\n, \\n, \\n, When we talk about the Ma...   \n",
       "4  Cultural Studies   [\\n, \\n, \\n, \\n, \\n, The culture of the educat...   \n",
       "\n",
       "                                              title  student  \n",
       "0  Impact of E-commerce on the Hospitality Industry        1  \n",
       "1                   Importance of Elasticity Demand        1  \n",
       "2     Posters of the Second World War - An Analysis        1  \n",
       "3                  Mathematical description of OFDM        1  \n",
       "4                     Cultural Analysis of Pakistan        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This let's me iterate faster by deleting it at the beginning\n",
    "import os\n",
    "os.remove('../firstpage.json')\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class EssayItem(scrapy.Item):\n",
    "    subject = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    text = scrapy.Field()\n",
    "\n",
    "class SESpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"SES\" # for student essay spider\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.ukessays.com/essays/',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response from the start_urls we declared.\n",
    "    def parse(self, response):\n",
    "\n",
    "        # Iterate over every column element on the page.\n",
    "        for subjects in response.xpath('//div[@class=\"col-lg-3 col-sm-6\"]/div/ul/li'):\n",
    "                \n",
    "            # Create our subject from this top-level page\n",
    "            subject = subjects.xpath('a/text()').extract_first()\n",
    "            \n",
    "            # Create an EssayItem called essay_item, empty for now\n",
    "            essay_item = EssayItem()\n",
    "            \n",
    "            # Create new url for parse_essays to use\n",
    "            essay_list_url = response.urljoin(subjects.xpath('a/@href').extract_first())\n",
    "            \n",
    "            # New request with essay_list_url, pass in our essay_item\n",
    "            yield scrapy.Request(essay_list_url, callback=self.parse_essays, \n",
    "                                 dont_filter=True,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject})\n",
    "            \n",
    "    def parse_essays(self, response):\n",
    "\n",
    "        # Retrieve essay items from metadata\n",
    "        essay_item = response.request.meta['item']\n",
    "        subject = response.request.meta['subject']\n",
    "        \n",
    "        for i, essay in enumerate(response.xpath('//div/article/div/h4')):\n",
    "            # Don't break the nice people's website\n",
    "            if i > 5:\n",
    "                break\n",
    "                \n",
    "            # Xpath to the essay's title\n",
    "            title = essay.xpath('a/text()').extract_first()\n",
    "            \n",
    "            # Xpath into the actual essay's link... finally!\n",
    "            essay_url = response.urljoin(essay.xpath('a/@href').extract_first())\n",
    "            \n",
    "            # This one should return the actual text, along with the rest of the item fields\n",
    "            yield scrapy.Request(essay_url, callback=self.collect_essay,\n",
    "                                 dont_filter=False,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject,\n",
    "                                       'title':title})\n",
    "        \n",
    "    def collect_essay(self, response):\n",
    "        # Retrieve our essay_item, once again, from Response's metadata\n",
    "        essay_item = response.request.meta['item']\n",
    "                \n",
    "        # Add the text, subject, and title into our essay_item\n",
    "        # Something weird was happening, probably due to the aysync nature of this stuff\n",
    "        essay_item['text'] = response.xpath('//body/main/div/div/article/p/text()').extract()\n",
    "        essay_item['subject'] = response.request.meta['subject']\n",
    "        essay_item['title'] = response.request.meta['title']        \n",
    "        \n",
    "        yield essay_item\n",
    "        \n",
    "        \n",
    "        \n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': '../firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False,           # Turn off logging for now.\n",
    "    'AUTOTHROTTLE_ENABLED' : False,\n",
    "    'HTTPCACHE_ENABLED' : False,\n",
    "    'ROBOTSTXT_ENABLED' : False,\n",
    "    'DOWNLOAD_DELAY' : 1.5\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(SESpider)\n",
    "process.start()\n",
    "print('Success!')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "uk_students = pd.read_json('../firstpage.json', orient='records', encoding='latin')\n",
    "uk_students['student'] = 1\n",
    "print(uk_students.shape)\n",
    "uk_students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(uk_students, open('../uk_students.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo!! All done with that one. On the the harder part: finding proffessional samples. The websites I've found generally do not post very many samples, unlike this UK Student Sample Bank. There were tens of thousands of essays to choose from, but the same website only posted 40 or so samples from their professionals. I will have to manually find a bunch of websites and make scrapers for all of them :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
