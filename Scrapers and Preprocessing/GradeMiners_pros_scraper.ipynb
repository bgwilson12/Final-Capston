{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "(59, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n                        , Introduction, \\r\\...</td>\n",
       "      <td>Patient safety in the operating room</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n                        , Statement of the ...</td>\n",
       "      <td>Pain Management and Alternative Therapies</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n                        The purpose of vent...</td>\n",
       "      <td>The purpose of ventilatory management</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>[\\n                        Family owned busine...</td>\n",
       "      <td>Family Business Succession</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>[\\n                        , Business Case Ana...</td>\n",
       "      <td>Business case report essay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject                                               text  \\\n",
       "0   Nursing  [\\n                        , Introduction, \\r\\...   \n",
       "1   Nursing  [\\n                        , Statement of the ...   \n",
       "2   Nursing  [\\n                        The purpose of vent...   \n",
       "3  Business  [\\n                        Family owned busine...   \n",
       "4  Business  [\\n                        , Business Case Ana...   \n",
       "\n",
       "                                       title  student  \n",
       "0       Patient safety in the operating room        0  \n",
       "1  Pain Management and Alternative Therapies        0  \n",
       "2      The purpose of ventilatory management        0  \n",
       "3                 Family Business Succession        0  \n",
       "4                 Business case report essay        0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This let's me iterate faster by deleting it at the beginning\n",
    "import os\n",
    "os.remove('../grademiner_pro.json')\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class EssayItem(scrapy.Item):\n",
    "    subject = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    text = scrapy.Field()\n",
    "\n",
    "class GMSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"GMS\" # for student essay spider\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://grademiners.com/free-papers',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response from the start_urls we declared.\n",
    "    def parse(self, response):\n",
    "        # Only pull essays from the \"Big Three\"\n",
    "        subject_list = ['Business', 'Law', 'Nursing']\n",
    "        \n",
    "        # Iterate over every column element on the page.\n",
    "        for subjects in response.xpath('//div[@class=\"col-xs-9\"]/h2[@class=\"subjects__item-title\"]/a'):\n",
    "            \n",
    "            if subjects.xpath('text()').extract_first() not in subject_list:\n",
    "                continue\n",
    "                \n",
    "            # Create our subject from this top-level page\n",
    "            subject = subjects.xpath('text()').extract_first()\n",
    "            \n",
    "            # Create an EssayItem called essay_item, empty for now\n",
    "            essay_item = EssayItem()\n",
    "            \n",
    "            # Create new url for parse_essays to use\n",
    "            essay_list_url = subjects.xpath('@href').extract_first()\n",
    "                        \n",
    "            # New request with essay_list_url, pass in our essay_item\n",
    "            yield scrapy.Request(essay_list_url, callback=self.parse_essays, \n",
    "                                 dont_filter=True,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject})\n",
    "            \n",
    "    def parse_essays(self, response):\n",
    "\n",
    "        # Retrieve essay items from metadata\n",
    "        essay_item = response.request.meta['item']\n",
    "        subject = response.request.meta['subject']\n",
    "        \n",
    "        for essay in response.xpath('//h2[@class=\"samples__title\"]/a'):\n",
    "                \n",
    "            # Xpath to the essay's title\n",
    "            title = essay.xpath('text()').extract_first()\n",
    "            \n",
    "            # Xpath into the actual essay's link... finally!\n",
    "            essay_url = essay.xpath('@href').extract_first()\n",
    "#             yield {'title':title, 'url':essay_url}\n",
    "            \n",
    "            # This one should return the actual text, along with the rest of the item fields\n",
    "            yield scrapy.Request(essay_url, callback=self.collect_essay,\n",
    "                                 dont_filter=False,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject,\n",
    "                                       'title':title})\n",
    "            \n",
    "        # Get the next page\n",
    "        next_url = response.xpath('//a[@class=\"next page-numbers\"]/@href').extract_first()\n",
    "        yield scrapy.Request(next_url, callback=self.parse_essays,\n",
    "                                 dont_filter=False,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject})\n",
    "        \n",
    "        \n",
    "    def collect_essay(self, response):\n",
    "\n",
    "        # Collect the first paragraph from the page\n",
    "        paragraph = response.xpath('//div[@class=\"sample__content-wrap\"]//text()').extract()\n",
    "#         yield {'text':paragraph}\n",
    "        # Retrieve our essay_item, once again, from Response's metadata\n",
    "        essay_item = response.request.meta['item']\n",
    "                \n",
    "        # Add the text, subject, and title into our essay_item\n",
    "        # Something weird was happening, probably due to the aysync nature of this stuff\n",
    "        essay_item['text'] = paragraph\n",
    "        essay_item['subject'] = response.request.meta['subject']\n",
    "        essay_item['title'] = response.request.meta['title']        \n",
    "        \n",
    "        yield essay_item\n",
    "        \n",
    "        \n",
    "        \n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT' : 'json',         # Store data in JSON format.\n",
    "    'FEED_URI' : '../grademiner_pro.json',  # Name our storage file.\n",
    "    'LOG_ENABLED' : False,           # Turn off logging for now.\n",
    "    'AUTOTHROTTLE_ENABLED' : True,\n",
    "    'HTTPCACHE_ENABLED' : True,\n",
    "    'ROBOTSTXT_ENABLED' : False,\n",
    "    'DOWNLOAD_DELAY' : 2\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(GMSpider)\n",
    "process.start()\n",
    "print('Success!')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "grademiner_pro = pd.read_json('../grademiner_pro.json', orient='records', encoding='latin')\n",
    "grademiner_pro['student'] = 0\n",
    "print(grademiner_pro.shape)\n",
    "grademiner_pro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(grademiner_pro, open('../grademiner_pro.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo!! All done with that one. On the the harder part: finding proffessional samples. The websites I've found generally do not post very many samples, unlike this UK Student Sample Bank. There were tens of thousands of essays to choose from, but the same website only posted 40 or so samples from their professionals. I will have to manually find a bunch of websites and make scrapers for all of them :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
