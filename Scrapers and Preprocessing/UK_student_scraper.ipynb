{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "(734, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, This paper will first com...</td>\n",
       "      <td>Limited Liability Corporations: Advantages and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Efficient, cost effective...</td>\n",
       "      <td>Bio-Medical Model of Health: History, Overview...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, What if there was an oppo...</td>\n",
       "      <td>Stem Cell Research and Utilitarianism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Leadership has been descr...</td>\n",
       "      <td>Motivation Skills Development Plan for Nursing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nursing</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Total Word Count: , This ...</td>\n",
       "      <td>Inquiry into Patient Death</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject                                               text  \\\n",
       "0  Business   [\\n, \\n, \\n, \\n, \\n, This paper will first com...   \n",
       "1   Nursing   [\\n, \\n, \\n, \\n, \\n, Efficient, cost effective...   \n",
       "2   Nursing   [\\n, \\n, \\n, \\n, \\n, What if there was an oppo...   \n",
       "3   Nursing   [\\n, \\n, \\n, \\n, \\n, Leadership has been descr...   \n",
       "4   Nursing   [\\n, \\n, \\n, \\n, \\n, Total Word Count: , This ...   \n",
       "\n",
       "                                               title  student  \n",
       "0  Limited Liability Corporations: Advantages and...        1  \n",
       "1  Bio-Medical Model of Health: History, Overview...        1  \n",
       "2              Stem Cell Research and Utilitarianism        1  \n",
       "3     Motivation Skills Development Plan for Nursing        1  \n",
       "4                         Inquiry into Patient Death        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This let's me iterate faster by deleting it at the beginning\n",
    "import os\n",
    "os.remove('../uk_students.json')\n",
    "\n",
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class EssayItem(scrapy.Item):\n",
    "    subject = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    text = scrapy.Field()\n",
    "\n",
    "class SESpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"SES\" # for student essay spider\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.ukessays.com/essays/',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response from the start_urls we declared.\n",
    "    def parse(self, response):\n",
    "        # Only pull essays from the \"Big Three\"\n",
    "        subject_list = ['Business ', 'Law ', 'Nursing ']\n",
    "        \n",
    "        # Iterate over every column element on the page.\n",
    "        for subjects in response.xpath('//div[@class=\"col-lg-3 col-sm-6\"]/div/ul/li'):\n",
    "            \n",
    "            if subjects.xpath('a/text()').extract_first() not in subject_list:\n",
    "                continue\n",
    "                \n",
    "            # Create our subject from this top-level page\n",
    "            subject = subjects.xpath('a/text()').extract_first()\n",
    "            \n",
    "            # Create an EssayItem called essay_item, empty for now\n",
    "            essay_item = EssayItem()\n",
    "            \n",
    "            # Create new url for parse_essays to use\n",
    "            essay_list_url = response.urljoin(subjects.xpath('a/@href').extract_first())\n",
    "            \n",
    "            # New request with essay_list_url, pass in our essay_item\n",
    "            yield scrapy.Request(essay_list_url, callback=self.parse_essays, \n",
    "                                 dont_filter=True,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject})\n",
    "            \n",
    "    def parse_essays(self, response):\n",
    "\n",
    "        # Retrieve essay items from metadata\n",
    "        essay_item = response.request.meta['item']\n",
    "        subject = response.request.meta['subject']\n",
    "        \n",
    "        for i, essay in enumerate(response.xpath('//div/article/div/h4')):\n",
    "                \n",
    "            # Xpath to the essay's title\n",
    "            title = essay.xpath('a/text()').extract_first()\n",
    "            \n",
    "            # Xpath into the actual essay's link... finally!\n",
    "            essay_url = response.urljoin(essay.xpath('a/@href').extract_first())\n",
    "            \n",
    "            # This one should return the actual text, along with the rest of the item fields\n",
    "            yield scrapy.Request(essay_url, callback=self.collect_essay,\n",
    "                                 dont_filter=False,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject,\n",
    "                                       'title':title})\n",
    "            \n",
    "        # After loop, update the page number\n",
    "        page_num = int(response.xpath('//li[@class=\"page-item active\"]/span/text()').extract_first())\n",
    "        \n",
    "        if page_num < 7:\n",
    "            next_page = '?page={}'.format(page_num+1)\n",
    "            next_url = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_url, callback=self.parse_essays,\n",
    "                                 dont_filter=False,\n",
    "                                 meta={'item':essay_item,\n",
    "                                       'subject':subject})\n",
    "        \n",
    "        \n",
    "    def collect_essay(self, response):\n",
    "        # Retrieve our essay_item, once again, from Response's metadata\n",
    "        essay_item = response.request.meta['item']\n",
    "                \n",
    "        # Add the text, subject, and title into our essay_item\n",
    "        # Something weird was happening, probably due to the aysync nature of this stuff\n",
    "        essay_item['text'] = response.xpath('//body/main/div/div/article/p/text()').extract()\n",
    "        essay_item['subject'] = response.request.meta['subject']\n",
    "        essay_item['title'] = response.request.meta['title']        \n",
    "        \n",
    "        yield essay_item\n",
    "        \n",
    "        \n",
    "        \n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT' : 'json',         # Store data in JSON format.\n",
    "    'FEED_URI' : '../uk_students.json',  # Name our storage file.\n",
    "    'LOG_ENABLED' : False,           # Turn off logging for now.\n",
    "    'AUTOTHROTTLE_ENABLED' : True,\n",
    "    'HTTPCACHE_ENABLED' : True,\n",
    "    'ROBOTSTXT_ENABLED' : False,\n",
    "    'DOWNLOAD_DELAY' : 2\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(SESpider)\n",
    "process.start()\n",
    "print('Success!')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "uk_students = pd.read_json('../uk_students.json', orient='records', encoding='latin')\n",
    "uk_students['student'] = 1\n",
    "print(uk_students.shape)\n",
    "uk_students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>student</th>\n",
       "      <th>joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Law</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, The primary scope of this...</td>\n",
       "      <td>Tenancy Agreement Problem Question</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n \\n \\n \\n \\n The primary scope of this paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Law</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Money laundering is a men...</td>\n",
       "      <td>Legislation and Regulation for Money Laundering</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n \\n \\n \\n \\n Money laundering is a menace. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, The dissertation is going...</td>\n",
       "      <td>Legal and Practical Impact of Insolvency</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n \\n \\n \\n \\n The dissertation is going to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Law</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, Brief 211716 The Rule of ...</td>\n",
       "      <td>British Concepts of the 'Rule of Law'</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n \\n \\n \\n \\n Brief 211716 The Rule of Law: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Law</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, In recent years, the Euro...</td>\n",
       "      <td>Impact of the European Directive 98/71/EC</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n \\n \\n \\n \\n In recent years, the European C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject                                               text  \\\n",
       "0    Law   [\\n, \\n, \\n, \\n, \\n, The primary scope of this...   \n",
       "1    Law   [\\n, \\n, \\n, \\n, \\n, Money laundering is a men...   \n",
       "2    Law   [\\n, \\n, \\n, \\n, \\n, The dissertation is going...   \n",
       "3    Law   [\\n, \\n, \\n, \\n, \\n, Brief 211716 The Rule of ...   \n",
       "4    Law   [\\n, \\n, \\n, \\n, \\n, In recent years, the Euro...   \n",
       "\n",
       "                                             title  student  \\\n",
       "0               Tenancy Agreement Problem Question        1   \n",
       "1  Legislation and Regulation for Money Laundering        1   \n",
       "2         Legal and Practical Impact of Insolvency        1   \n",
       "3            British Concepts of the 'Rule of Law'        1   \n",
       "4        Impact of the European Directive 98/71/EC        1   \n",
       "\n",
       "                                              joined  \n",
       "0  \\n \\n \\n \\n \\n The primary scope of this paper...  \n",
       "1  \\n \\n \\n \\n \\n Money laundering is a menace. T...  \n",
       "2  \\n \\n \\n \\n \\n The dissertation is going to co...  \n",
       "3  \\n \\n \\n \\n \\n Brief 211716 The Rule of Law: W...  \n",
       "4  \\n \\n \\n \\n \\n In recent years, the European C...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspector = uk_students.copy()\n",
    "inspector['joined'] = inspector.text.map(lambda x: ' '.join(x))\n",
    "inspect_law = inspector[inspector['subject'] == 'Law ']\n",
    "inspect_law.reset_index(inplace=True, drop=True)\n",
    "inspect_law.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\\n \\n \\n \\n \\n To export a reference to this article please select a referencing stye below:                 112\n",
       "\\n \\n \\n \\n \\n b) Secondly there is   This deterrence focuses on making examples of perpetrators. It is n      1\n",
       "\\n \\n \\n \\n \\n A burglary has occurred around 21:00 hours yesterday at an office complex on an industrial      1\n",
       "\\n \\n \\n \\n \\n In a society that often prides itself on having a free and unbiased media, it is important      1\n",
       "\\n \\n \\n \\n \\n “Dispute Resolution at the Workplace: The Practical, procedural and legal aspects”. Analys      1\n",
       "\\n \\n \\n \\n \\n The development of contract law into its modern conception is fundamentally based on the L      1\n",
       "\\n \\n \\n \\n \\n A mortgage is a contract between two parties whereby the mortgagor uses his land as securi      1\n",
       "\\n \\n \\n \\n \\n In this scenario, the first significant point is the nature of the parties’ respective fir      1\n",
       "\\n \\n \\n \\n \\n This question is on negligence and it involves the following: duty of care; delictual liab      1\n",
       "\\n \\n \\n \\n \\n Law is a “  Hon. Benjamin N Cardozo . The work of deciding cases goes on every day in hund      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([each[:100] for each in inspect_law.joined]).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(uk_students, open('../uk_students.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo!! All done with that one. On the the harder part: finding proffessional samples. The websites I've found generally do not post very many samples, unlike this UK Student Sample Bank. There were tens of thousands of essays to choose from, but the same website only posted 40 or so samples from their professionals. I will have to manually find a bunch of websites and make scrapers for all of them :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
